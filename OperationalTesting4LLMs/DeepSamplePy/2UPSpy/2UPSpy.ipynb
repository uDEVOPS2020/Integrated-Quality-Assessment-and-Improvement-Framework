{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "946fe903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Confidence_Score_50.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Confidence_Score_100.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Confidence_Score_200.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Confidence_Score_400.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Confidence_Score_800.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Prediction_Entropy_50.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Prediction_Entropy_100.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Prediction_Entropy_200.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Prediction_Entropy_400.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Prediction_Entropy_800.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Similarity_Score_50.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Similarity_Score_100.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Similarity_Score_200.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Similarity_Score_400.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_Similarity_Score_800.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_DSA_50.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_DSA_100.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_DSA_200.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_DSA_400.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_DSA_800.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_LSA_50.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_LSA_100.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_LSA_200.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_LSA_400.txt\n",
      "Sampled data saved to 2UPSpy_results/imdb300AuxDS_LSA_800.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Confidence_Score_50.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Confidence_Score_100.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Confidence_Score_200.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Confidence_Score_400.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Confidence_Score_800.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Prediction_Entropy_50.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Prediction_Entropy_100.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Prediction_Entropy_200.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Prediction_Entropy_400.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Prediction_Entropy_800.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Similarity_Score_50.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Similarity_Score_100.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Similarity_Score_200.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Similarity_Score_400.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_Similarity_Score_800.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_DSA_50.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_DSA_100.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_DSA_200.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_DSA_400.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_DSA_800.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_LSA_50.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_LSA_100.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_LSA_200.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_LSA_400.txt\n",
      "Sampled data saved to 2UPSpy_results/imdbAuxDS_LSA_800.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Confidence_Score_50.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Confidence_Score_100.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Confidence_Score_200.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Confidence_Score_400.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Confidence_Score_800.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Prediction_Entropy_50.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Prediction_Entropy_100.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Prediction_Entropy_200.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Prediction_Entropy_400.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Prediction_Entropy_800.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Similarity_Score_50.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Similarity_Score_100.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Similarity_Score_200.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Similarity_Score_400.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_Similarity_Score_800.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_DSA_50.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_DSA_100.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_DSA_200.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_DSA_400.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_DSA_800.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_LSA_50.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_LSA_100.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_LSA_200.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_LSA_400.txt\n",
      "Sampled data saved to 2UPSpy_results/SSTtestAuxDS_LSA_800.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define datasets, auxiliary variables, and budgets\n",
    "datasets = [\"imdb300AuxDS.csv\", \"imdbAuxDS.csv\", \"SSTtestAuxDS.csv\"]\n",
    "aux_variables = [\"Confidence_Score\", \"Prediction_Entropy\", \"Similarity_Score\", \"DSA\", \"LSA\"]\n",
    "budgets = [50, 100, 200, 400, 800]\n",
    "root_dir = \"../dataset\"\n",
    "output_dir = \"2UPSpy_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "def load_data(filename):\n",
    "    df = pd.read_csv(os.path.join(root_dir, filename))\n",
    "    df['Outcome'] = df['Outcome'].apply(lambda x: 1 if x.lower() == 'pass' else 0)  # Convert 'Outcome' to binary for consistency\n",
    "    return df\n",
    "\n",
    "# Calculate partition probabilities based on auxiliary variables\n",
    "def calculate_partition_probabilities(df, aux_variable, num_partitions):\n",
    "    df['partition'] = pd.qcut(df[aux_variable], q=num_partitions, labels=False, duplicates='drop')\n",
    "    total_sum = df[aux_variable].sum()\n",
    "    df['prob'] = df.groupby('partition')[aux_variable].transform('sum') / total_sum  # Normalize sum per partition\n",
    "    partition_probs = df.groupby('partition')['prob'].first().values  # Probabilities for each partition\n",
    "    return partition_probs, df['partition'].unique()\n",
    "\n",
    "# Two-Stage Unequal Probability Sampling\n",
    "def two_stage_sampling(df, aux_variable, num_partitions=10, budget=100):\n",
    "    partition_probs, partitions = calculate_partition_probabilities(df, aux_variable, num_partitions)\n",
    "    selected_partitions = np.random.choice(partitions, size=budget, p=partition_probs, replace=True)\n",
    "\n",
    "    samples = []\n",
    "    for partition in selected_partitions:\n",
    "        partition_data = df[df['partition'] == partition]\n",
    "        if not partition_data.empty:\n",
    "            sample = partition_data.sample(n=1, replace=False)\n",
    "            samples.append(sample)\n",
    "\n",
    "    return pd.concat(samples) if samples else pd.DataFrame()\n",
    "\n",
    "# Process and save results for each configuration\n",
    "for dataset in datasets:\n",
    "    df = load_data(dataset)\n",
    "    for aux_var in aux_variables:\n",
    "        for budget in budgets:\n",
    "            output_filename = f\"{output_dir}/{dataset[:-4]}_{aux_var}_{budget}.txt\"\n",
    "            with open(output_filename, 'w') as file:\n",
    "                file.write(\"accuracy,failures\\n\")\n",
    "                for _ in range(30):  # Perform sampling 30 times for robust statistics\n",
    "                    sampled_data = two_stage_sampling(df, aux_var, num_partitions=10, budget=budget)\n",
    "                    # Calculate accuracy and count failures\n",
    "                    failures = sampled_data['Outcome'].value_counts().get(0, 0)\n",
    "                    total = len(sampled_data)\n",
    "                    accuracy = (total - failures) / total if total > 0 else 0\n",
    "                    file.write(f\"{accuracy},{failures}\\n\")\n",
    "            print(f\"Sampled data saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e477d247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Confidence_Score_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Confidence_Score_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Confidence_Score_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Confidence_Score_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Confidence_Score_800.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Prediction_Entropy_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Prediction_Entropy_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Prediction_Entropy_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Prediction_Entropy_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Prediction_Entropy_800.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Similarity_Score_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Similarity_Score_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Similarity_Score_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Similarity_Score_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_Similarity_Score_800.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_DSA_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_DSA_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_DSA_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_DSA_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_DSA_800.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_LSA_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_LSA_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_LSA_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_LSA_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdb300AuxDS_LSA_800.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Confidence_Score_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Confidence_Score_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Confidence_Score_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Confidence_Score_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Confidence_Score_800.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Prediction_Entropy_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Prediction_Entropy_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Prediction_Entropy_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Prediction_Entropy_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Prediction_Entropy_800.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Similarity_Score_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Similarity_Score_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Similarity_Score_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Similarity_Score_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_Similarity_Score_800.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_DSA_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_DSA_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_DSA_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_DSA_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_DSA_800.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_LSA_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_LSA_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_LSA_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_LSA_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/imdbAuxDS_LSA_800.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Confidence_Score_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Confidence_Score_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Confidence_Score_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Confidence_Score_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Confidence_Score_800.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Prediction_Entropy_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Prediction_Entropy_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Prediction_Entropy_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Prediction_Entropy_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Prediction_Entropy_800.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Similarity_Score_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Similarity_Score_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Similarity_Score_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Similarity_Score_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_Similarity_Score_800.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_DSA_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_DSA_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_DSA_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_DSA_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_DSA_800.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_LSA_50.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_LSA_100.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_LSA_200.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_LSA_400.txt\n",
      "Sampled data saved to TwoUPSSamplingResults/SSTtestAuxDS_LSA_800.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Setup environment\n",
    "datasets = [\"imdb300AuxDS.csv\", \"imdbAuxDS.csv\", \"SSTtestAuxDS.csv\"]\n",
    "aux_variables = [\"Confidence_Score\", \"Prediction_Entropy\", \"Similarity_Score\", \"DSA\", \"LSA\"]\n",
    "budgets = [50, 100, 200, 400, 800]\n",
    "root_dir = \"../dataset\"\n",
    "output_dir = \"TwoUPSSamplingResults\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def load_data(filename):\n",
    "    df = pd.read_csv(os.path.join(root_dir, filename))\n",
    "    df['Outcome'] = df['Outcome'].apply(lambda x: 1 if x.lower() == 'pass' else 0)\n",
    "    return df\n",
    "\n",
    "def calculate_partition_probabilities(df, aux_variable, num_partitions):\n",
    "    df['partition'] = pd.qcut(df[aux_variable], q=num_partitions, labels=False, duplicates='drop')\n",
    "    partition_sums = df.groupby('partition')[aux_variable].sum()\n",
    "    total_sum = partition_sums.sum()\n",
    "    partition_probabilities = partition_sums / total_sum\n",
    "    return partition_probabilities, df['partition'].unique()\n",
    "\n",
    "def two_stage_sampling(df, aux_variable, num_partitions=10, budget=100):\n",
    "    partition_probabilities, partitions = calculate_partition_probabilities(df, aux_variable, num_partitions)\n",
    "    selected_partitions = np.random.choice(partitions, size=budget, p=partition_probabilities.values, replace=True)\n",
    "\n",
    "    samples = []\n",
    "    for partition in selected_partitions:\n",
    "        partition_data = df[df['partition'] == partition]\n",
    "        if not partition_data.empty:\n",
    "            # Increase likelihood of sampling failures if they are underrepresented\n",
    "            if partition_data['Outcome'].mean() < 0.5:  # More failures than successes\n",
    "                sample = partition_data.sample(n=1, replace=False, weights='Outcome' if partition_data['Outcome'].sum() > 0 else None)\n",
    "            else:\n",
    "                sample = partition_data.sample(n=1, replace=False)\n",
    "            samples.append(sample)\n",
    "\n",
    "    sampled_data = pd.concat(samples) if samples else pd.DataFrame()\n",
    "    if not sampled_data.empty:\n",
    "        sampled_data['weight'] = 1 / partition_probabilities[sampled_data['partition']].values\n",
    "        sampled_data['weight'] *= (budget / sampled_data['weight'].sum())\n",
    "    return sampled_data\n",
    "\n",
    "def calculate_hansen_hurwitz_estimators(samples):\n",
    "    if not samples.empty:\n",
    "        weighted_failures = np.sum((1 - samples['Outcome']) * samples['weight'])\n",
    "        total_weight = np.sum(samples['weight'])\n",
    "        accuracy = np.sum(samples['Outcome'] * samples['weight']) / total_weight if total_weight > 0 else 0\n",
    "        return accuracy, int(weighted_failures)\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = load_data(dataset)\n",
    "    for aux_var in aux_variables:\n",
    "        for budget in budgets:\n",
    "            output_filename = f\"{output_dir}/{dataset[:-4]}_{aux_var}_{budget}.txt\"\n",
    "            with open(output_filename, 'w') as file:\n",
    "                file.write(\"accuracy,failures\\n\")\n",
    "                for _ in range(30):\n",
    "                    sampled_data = two_stage_sampling(df, aux_var, num_partitions=10, budget=budget)\n",
    "                    accuracy, failures = calculate_hansen_hurwitz_estimators(sampled_data)\n",
    "                    file.write(f\"{accuracy},{failures}\\n\")\n",
    "            print(f\"Sampled data saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7bb41d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Confidence_Score_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Confidence_Score_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Confidence_Score_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Confidence_Score_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Confidence_Score_800.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Prediction_Entropy_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Prediction_Entropy_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Prediction_Entropy_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Prediction_Entropy_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Prediction_Entropy_800.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Similarity_Score_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Similarity_Score_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Similarity_Score_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Similarity_Score_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_Similarity_Score_800.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_DSA_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_DSA_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_DSA_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_DSA_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_DSA_800.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_LSA_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_LSA_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_LSA_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_LSA_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdb300AuxDS_LSA_800.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Confidence_Score_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Confidence_Score_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Confidence_Score_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Confidence_Score_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Confidence_Score_800.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Prediction_Entropy_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Prediction_Entropy_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Prediction_Entropy_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Prediction_Entropy_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Prediction_Entropy_800.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Similarity_Score_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Similarity_Score_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Similarity_Score_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Similarity_Score_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_Similarity_Score_800.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_DSA_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_DSA_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_DSA_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_DSA_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_DSA_800.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_LSA_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_LSA_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_LSA_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_LSA_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/imdbAuxDS_LSA_800.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Confidence_Score_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Confidence_Score_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Confidence_Score_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Confidence_Score_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Confidence_Score_800.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Prediction_Entropy_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Prediction_Entropy_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Prediction_Entropy_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Prediction_Entropy_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Prediction_Entropy_800.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Similarity_Score_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Similarity_Score_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Similarity_Score_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Similarity_Score_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_Similarity_Score_800.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_DSA_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_DSA_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_DSA_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_DSA_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_DSA_800.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_LSA_50.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_LSA_100.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_LSA_200.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_LSA_400.txt\n",
      "Sampled data saved to 2UPS_resultsV3/SSTtestAuxDS_LSA_800.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Setup environment\n",
    "datasets = [\"imdb300AuxDS.csv\", \"imdbAuxDS.csv\", \"SSTtestAuxDS.csv\"]\n",
    "aux_variables = [\"Confidence_Score\", \"Prediction_Entropy\", \"Similarity_Score\", \"DSA\", \"LSA\"]\n",
    "budgets = [50, 100, 200, 400, 800]\n",
    "root_dir = \"../dataset\"\n",
    "output_dir = \"2UPS_resultsV3\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def load_data(filename):\n",
    "    df = pd.read_csv(os.path.join(root_dir, filename))\n",
    "    df['Outcome'] = df['Outcome'].apply(lambda x: 1 if x.lower() == 'pass' else 0)\n",
    "    return df\n",
    "\n",
    "def calculate_partition_probabilities(df, aux_variable, num_partitions):\n",
    "    df['partition'] = pd.qcut(df[aux_variable], q=num_partitions, labels=False, duplicates='drop')\n",
    "    partition_sums = df.groupby('partition')[aux_variable].sum()\n",
    "    total_sum = partition_sums.sum()\n",
    "    partition_probabilities = partition_sums / total_sum\n",
    "    return partition_probabilities, df['partition'].unique()\n",
    "\n",
    "\n",
    "\n",
    "def two_stage_sampling(df, aux_variable, num_partitions=10, budget=100):\n",
    "    epsilon = 1e-8  # Small number to prevent division by zero\n",
    "    partition_probabilities, partitions = calculate_partition_probabilities(df, aux_variable, num_partitions)\n",
    "    selected_partitions = np.random.choice(partitions, size=budget, p=partition_probabilities.values, replace=True)\n",
    "\n",
    "    samples = []\n",
    "    for partition in selected_partitions:\n",
    "        partition_data = df[df['partition'] == partition]\n",
    "        if not partition_data.empty:\n",
    "            # Adjusting weights for aggressive handling of class imbalance\n",
    "            if partition_data['Outcome'].mean() == 1:\n",
    "                weights = np.where(partition_data['Outcome'] == 0, 1 / (1 - partition_data['Outcome'].mean() + epsilon), np.ones(len(partition_data)))\n",
    "            elif partition_data['Outcome'].mean() == 0:\n",
    "                weights = np.where(partition_data['Outcome'] == 0, np.ones(len(partition_data)), 1 / (partition_data['Outcome'].mean() + epsilon))\n",
    "            else:\n",
    "                weights = np.where(partition_data['Outcome'] == 0, 1 / (1 - partition_data['Outcome'].mean()), 1 / partition_data['Outcome'].mean())\n",
    "            sample = partition_data.sample(n=1, replace=False, weights=weights)\n",
    "            samples.append(sample)\n",
    "\n",
    "    sampled_data = pd.concat(samples) if samples else pd.DataFrame()\n",
    "    if not sampled_data.empty:\n",
    "        sampled_data['weight'] = 1 / partition_probabilities[sampled_data['partition']].values\n",
    "        sampled_data['weight'] *= (budget / sampled_data['weight'].sum())\n",
    "    return sampled_data\n",
    "\n",
    "def calculate_hansen_hurwitz_estimators(samples):\n",
    "    if not samples.empty:\n",
    "        weighted_failures = np.sum((1 - samples['Outcome']) * samples['weight'])\n",
    "        total_weight = np.sum(samples['weight'])\n",
    "        accuracy = np.sum(samples['Outcome'] * samples['weight']) / total_weight if total_weight > 0 else 0\n",
    "        return accuracy, int(weighted_failures)\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = load_data(dataset)\n",
    "    for aux_var in aux_variables:\n",
    "        for budget in budgets:\n",
    "            output_filename = f\"{output_dir}/{dataset[:-4]}_{aux_var}_{budget}.txt\"\n",
    "            with open(output_filename, 'w') as file:\n",
    "                file.write(\"accuracy,failures\\n\")\n",
    "                for _ in range(30):\n",
    "                    sampled_data = two_stage_sampling(df, aux_var, num_partitions=10, budget=budget)\n",
    "                    accuracy, failures = calculate_hansen_hurwitz_estimators(sampled_data)\n",
    "                    file.write(f\"{accuracy},{failures}\\n\")\n",
    "            print(f\"Sampled data saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da23761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
