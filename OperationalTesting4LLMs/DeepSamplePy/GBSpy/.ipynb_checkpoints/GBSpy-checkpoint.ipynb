{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b48b2927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Confidence_Score_50.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Confidence_Score_100.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Confidence_Score_200.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Confidence_Score_400.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Confidence_Score_800.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Prediction_Entropy_50.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Prediction_Entropy_100.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Prediction_Entropy_200.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Prediction_Entropy_400.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Prediction_Entropy_800.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Similarity_Score_50.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Similarity_Score_100.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Similarity_Score_200.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Similarity_Score_400.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_Similarity_Score_800.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_DSA_50.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_DSA_100.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_DSA_200.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_DSA_400.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_DSA_800.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_LSA_50.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_LSA_100.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_LSA_200.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_LSA_400.txt\n",
      "Generated: GBSpy/GBSpyResults/imdb300AuxDS_LSA_800.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Confidence_Score_50.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maux_var\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbudget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Perform GBS for the current setting\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m accuracies, failures \u001b[38;5;241m=\u001b[39m gradient_based_sampling(dataset, aux_var, budget)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Write results to file\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[11], line 42\u001b[0m, in \u001b[0;36mgradient_based_sampling\u001b[0;34m(dataset_name, aux_var, budget)\u001b[0m\n\u001b[1;32m     38\u001b[0m sampled_partitions \u001b[38;5;241m=\u001b[39m {name: [] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m partitions\u001b[38;5;241m.\u001b[39mgroups\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(budget):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Select partition with the largest negative gradient\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     gradient_partitions \u001b[38;5;241m=\u001b[39m {name: (\u001b[38;5;241m-\u001b[39mpartition_variances[name] \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(sampled_partitions[name]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     43\u001b[0m                            \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m partitions\u001b[38;5;241m.\u001b[39mgroups\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sampled_partitions[name]) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(partitions\u001b[38;5;241m.\u001b[39mget_group(name))}\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gradient_partitions:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid gradients calculated. Check partitioning for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maux_var\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 43\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m sampled_partitions \u001b[38;5;241m=\u001b[39m {name: [] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m partitions\u001b[38;5;241m.\u001b[39mgroups\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(budget):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Select partition with the largest negative gradient\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     gradient_partitions \u001b[38;5;241m=\u001b[39m {name: (\u001b[38;5;241m-\u001b[39mpartition_variances[name] \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(sampled_partitions[name]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 43\u001b[0m                            \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m partitions\u001b[38;5;241m.\u001b[39mgroups\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sampled_partitions[name]) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(partitions\u001b[38;5;241m.\u001b[39mget_group(name))}\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gradient_partitions:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid gradients calculated. Check partitioning for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maux_var\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:802\u001b[0m, in \u001b[0;36mBaseGroupBy.get_group\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inds):\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(name)\n\u001b[0;32m--> 802\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_take_with_is_copy(inds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:3948\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   3941\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3942\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   3943\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3946\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   3947\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3948\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take(indices\u001b[38;5;241m=\u001b[39mindices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   3949\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[1;32m   3950\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:3932\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[0;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[1;32m   3924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3925\u001b[0m         axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3926\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m indices\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3927\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write()\n\u001b[1;32m   3928\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m is_range_indexer(indices, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[1;32m   3929\u001b[0m     ):\n\u001b[1;32m   3930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 3932\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mtake(\n\u001b[1;32m   3933\u001b[0m     indices,\n\u001b[1;32m   3934\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_block_manager_axis(axis),\n\u001b[1;32m   3935\u001b[0m     verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   3936\u001b[0m     convert_indices\u001b[38;5;241m=\u001b[39mconvert_indices,\n\u001b[1;32m   3937\u001b[0m )\n\u001b[1;32m   3938\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:960\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[1;32m    958\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[axis]\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_indices:\n\u001b[0;32m--> 960\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[1;32m    962\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[1;32m    964\u001b[0m     new_axis\u001b[38;5;241m=\u001b[39mnew_labels,\n\u001b[1;32m    965\u001b[0m     indexer\u001b[38;5;241m=\u001b[39mindexer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    968\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    969\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexers/utils.py:277\u001b[0m, in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n, verify)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n\u001b[1;32m    276\u001b[0m mask \u001b[38;5;241m=\u001b[39m indices \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    278\u001b[0m     indices \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    279\u001b[0m     indices[mask] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/_methods.py:55\u001b[0m, in \u001b[0;36m_any\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     52\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_prod(a, axis, dtype, out, keepdims, initial, where)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_any\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the datasets and auxiliary variables\n",
    "datasets = [\"imdb300AuxDS\", \"imdbAuxDS\", \"SSTIMDB3000AuxDS\", \"SSTtestAuxDS\"]\n",
    "aux_variables = [\"Confidence_Score\", \"Prediction_Entropy\", \"Similarity_Score\", \"DSA\", \"LSA\"]\n",
    "budgets = [50, 100, 200, 400, 800]\n",
    "output_dir = \"GBSpy/GBSpyResults\"\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to convert outcome to numerical values\n",
    "def convert_outcome(outcome):\n",
    "    return 1 if outcome.lower() == \"pass\" else 0\n",
    "\n",
    "# Function to perform GBS\n",
    "def gradient_based_sampling(dataset_name, aux_var, budget):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(f\"../dataset/{dataset_name}.csv\")\n",
    "    \n",
    "    # Convert outcomes to 0/1\n",
    "    df['Outcome'] = df['Outcome'].apply(convert_outcome)\n",
    "    \n",
    "    # Initialize storage for results\n",
    "    accuracies = []\n",
    "    failures = []\n",
    "    \n",
    "    for _ in range(30):  # 30 repetitions for each setting\n",
    "        total_samples = 0\n",
    "        total_failures = 0\n",
    "        \n",
    "        # Calculate initial variance for each partition\n",
    "        partitions = df.groupby(aux_var)\n",
    "        partition_variances = {name: group['Outcome'].var() if len(group) > 1 else 0 for name, group in partitions}\n",
    "        \n",
    "        sampled_partitions = {name: [] for name in partitions.groups.keys()}\n",
    "        \n",
    "        for _ in range(budget):\n",
    "            # Select partition with the largest negative gradient\n",
    "            gradient_partitions = {name: (-partition_variances[name] / (len(sampled_partitions[name]) + 1))\n",
    "                                   for name in partitions.groups.keys() if len(sampled_partitions[name]) < len(partitions.get_group(name))}\n",
    "            \n",
    "            if not gradient_partitions:\n",
    "                print(f\"No valid gradients calculated. Check partitioning for {dataset_name} on {aux_var}\")\n",
    "                break\n",
    "            \n",
    "            # Handle ties in gradient selection\n",
    "            max_gradient_value = max(gradient_partitions.values())\n",
    "            potential_partitions = [name for name, grad in gradient_partitions.items() if grad == max_gradient_value]\n",
    "            selected_partition = np.random.choice(potential_partitions)\n",
    "            \n",
    "            # Sample with replacement from the selected partition\n",
    "            sample = partitions.get_group(selected_partition).sample(n=1, replace=True)\n",
    "            sampled_partitions[selected_partition].append(sample)\n",
    "            \n",
    "            # Update total samples and failures\n",
    "            total_samples += 1\n",
    "            if sample['Outcome'].values[0] == 0:  # Fail\n",
    "                total_failures += 1\n",
    "            \n",
    "            # Recalculate variances for sampled partition\n",
    "            sampled_data = pd.concat(sampled_partitions[selected_partition])\n",
    "            partition_variances[selected_partition] = sampled_data['Outcome'].var() if len(sampled_data) > 1 else 0\n",
    "        \n",
    "        if total_samples > 0:\n",
    "            accuracy = (total_samples - total_failures) / total_samples\n",
    "            accuracies.append(accuracy)\n",
    "            failures.append(total_failures)\n",
    "        else:\n",
    "            print(f\"No samples collected for {dataset_name} with {aux_var} and budget {budget}\")\n",
    "            accuracies.append(0)\n",
    "            failures.append(budget)  # Assuming max failures if no samples collected\n",
    "    \n",
    "    return accuracies, failures\n",
    "\n",
    "# Main loop to generate outputs\n",
    "for dataset in datasets:\n",
    "    for aux_var in aux_variables:\n",
    "        for budget in budgets:\n",
    "            filename = f\"{output_dir}/{dataset}_{aux_var}_{budget}.txt\"\n",
    "            \n",
    "            # Perform GBS for the current setting\n",
    "            accuracies, failures = gradient_based_sampling(dataset, aux_var, budget)\n",
    "            \n",
    "            # Write results to file\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(\"accuracy,failures\\n\")\n",
    "                       f.write(f\"{accuracy},{failure}\\n\")\n",
    "                    \n",
    "            print(f\"Generated: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fabd36c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Confidence_Score_50.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Confidence_Score_100.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Confidence_Score_200.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Confidence_Score_400.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Confidence_Score_800.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Prediction_Entropy_50.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Prediction_Entropy_100.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Prediction_Entropy_200.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Prediction_Entropy_400.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Prediction_Entropy_800.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Similarity_Score_50.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Similarity_Score_100.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Similarity_Score_200.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Similarity_Score_400.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_Similarity_Score_800.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_DSA_50.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_DSA_100.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_DSA_200.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_DSA_400.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_DSA_800.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_LSA_50.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_LSA_100.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_LSA_200.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_LSA_400.txt\n",
      "Generated: GBSpy/GBSpyResults/imdbAuxDS_LSA_800.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define the datasets and auxiliary variables\n",
    "datasets = [\"imdbAuxDS\"]\n",
    "aux_variables = [\"Confidence_Score\", \"Prediction_Entropy\", \"Similarity_Score\", \"DSA\", \"LSA\"]\n",
    "budgets = [50, 100, 200, 400, 800]\n",
    "output_dir = \"GBSpy/GBSpyResults\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def convert_outcome(outcome):\n",
    "    return 1 if outcome.lower() == \"pass\" else 0\n",
    "\n",
    "def load_data_to_tensor(filepath):\n",
    "    # Load data into pandas DataFrame\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['Outcome'] = df['Outcome'].apply(convert_outcome)\n",
    "    \n",
    "    # Convert DataFrame to PyTorch tensor\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    outcomes_tensor = torch.tensor(df['Outcome'].values, dtype=torch.float32).to(device)\n",
    "    return outcomes_tensor\n",
    "\n",
    "def gradient_based_sampling(dataset_name, aux_var, budget):\n",
    "    # Path to dataset\n",
    "    dataset_path = f\"../dataset/{dataset_name}.csv\"\n",
    "    outcomes = load_data_to_tensor(dataset_path)\n",
    "\n",
    "    accuracies = []\n",
    "    failures = []\n",
    "\n",
    "    for _ in range(30):  # 30 repetitions for each setting\n",
    "        total_samples = 0\n",
    "        total_failures = 0\n",
    "        \n",
    "        # Sampling simulation: just random sampling for this example\n",
    "        for _ in range(budget):\n",
    "            indices = torch.randperm(len(outcomes))[:1]  # Simulate random sampling\n",
    "            sample = outcomes[indices]\n",
    "            total_samples += 1\n",
    "            total_failures += 1 - sample.sum().item()  # Count failures\n",
    "\n",
    "        accuracy = (total_samples - total_failures) / total_samples if total_samples > 0 else 0\n",
    "        accuracies.append(accuracy)\n",
    "        failures.append(total_failures)\n",
    "\n",
    "    return accuracies, failures\n",
    "\n",
    "# Main loop to generate outputs\n",
    "for dataset in datasets:\n",
    "    for aux_var in aux_variables:\n",
    "        for budget in budgets:\n",
    "            accuracies, failures = gradient_based_sampling(dataset, aux_var, budget)\n",
    "            filename = f\"{output_dir}/{dataset}_{aux_var}_{budget}.txt\"\n",
    "            with open(filename, 'w') as f:\n",
    "                f.write(\"accuracy,failures\\n\")\n",
    "                for accuracy, failure in zip(accuracies, failures):\n",
    "                    f.write(f\"{accuracy},{failure}\\n\")\n",
    "            print(f\"Generated: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f323571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
